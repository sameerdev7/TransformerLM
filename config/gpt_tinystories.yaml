# GPT Configuration for TinyStories
experiment_name: "gpt_tinystories"
description: "GPT model trained on TinyStories from HuggingFace"

model:
  vocab_size: 5000 # Number of descrete tokens a model can emit
  context_length: 256 # Max tokens model can see at once
  d_model: 384 # Width of all token representation
  num_layers: 6
  num_heads: 6
  d_ff: 1536  # 4 * d_model
  rope_theta: 10000.0

data:
  dataset_name: "roneneldan/TinyStories"
  train_split: "train"
  val_split: "validation"
  max_samples: null  # null = use all, or set to number like 50000
  processed_dir: "data/processed"
  tokenizer_dir: "data/tokenizer"

training:
  batch_size: 64 # Sequences per gradient step
  learning_rate: 3.0e-4
  min_lr: 1.0e-4
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  
  max_iters: 5000
  warmup_iters: 500
  
  log_interval: 50
  eval_interval: 250
  eval_iters: 100
  checkpoint_interval: 1000
  
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  
  seed: 42
